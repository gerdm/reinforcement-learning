{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab394903-07c2-4325-9189-80070f0f3350",
   "metadata": {},
   "source": [
    "# Blackjack - prediction\n",
    "\n",
    "In this notebook, we are interested in *learning* the value-function $v_\\pi(s)$ and action-value function $q_\\pi(s, a)$ for a given policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd929d34-5f36-4754-b653-f0f2171e6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "debf50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71627d2e",
   "metadata": {},
   "source": [
    "* **Stick**: Player stops\n",
    "* **Hit**: Request an additional card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f760e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = np.ones(10)\n",
    "# n_vals[-1] = 4\n",
    "deck_probs = n_vals / n_vals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cbb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core import types\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42d77a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def draw_card():\n",
    "    return np.random.multinomial(1, deck_probs).argmax()\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def dealer_strategy(value_cards):\n",
    "    \"\"\"\n",
    "    Dealer's fixed strategy\n",
    "    \"\"\"\n",
    "    while value_cards < 17:\n",
    "        value_cards = value_cards + draw_card()\n",
    "    return value_cards\n",
    "\n",
    "\n",
    "@jit(nopython=True)\n",
    "def update_player_card(current_value, new_card):\n",
    "    \"\"\"\n",
    "    Update the player's hand value.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    current_value: int\n",
    "        Current value of player's hand\n",
    "    new_card: int\n",
    "        Randomly-drawn card\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tuple: (int, bool)\n",
    "        1. New value of player's hand\n",
    "        2. Whether the there is a usable ace.\n",
    "    \"\"\"\n",
    "    has_usable_ace = False\n",
    "    if new_card == 1.0 and current_value <= 10:\n",
    "        new_card = 11\n",
    "        has_usable_ace = True\n",
    "        \n",
    "    new_value = current_value + new_card\n",
    "    return new_value, has_usable_ace\n",
    "\n",
    "@jit(nopython=True)\n",
    "def blackjack(player_value_cards, dealer_cards, policy, has_usable_ace):\n",
    "    \"\"\"\n",
    "    Evaluate a single play of Blackjack.\n",
    "    \n",
    "    For some reason, a player can only have a minimum value of 12\n",
    "    on her initial value cards.\n",
    "    \n",
    "    At the start of the game, we are given the initial value of the cards\n",
    "    of the player, the initial dealer cards and a policy for the player.\n",
    "    Furtheremore, we are given whether the player has a usable ace.\n",
    "    \n",
    "    Actions:\n",
    "        0: hit\n",
    "        1: stick\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    player_value_cards: float\n",
    "        Current value cards for the players\n",
    "    dealer_cards: jnp.array(2)\n",
    "        Dealer's initial cards\n",
    "    policy: jnp.array(G,A)\n",
    "        2d-array specifying if having value g ∈ G the player\n",
    "        should take action a ∈ A, i.e., policy[g,a] == 1.0\n",
    "        if action a should be taken if the value of the cards\n",
    "        is g.\n",
    "    has_usable_ace: bool\n",
    "        Wehether the initial player_value_cards contains a\n",
    "        usable ace.\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    \n",
    "    hist_reward = [reward]\n",
    "    hist_state = [(player_value_cards, has_usable_ace)]\n",
    "    hist_action = [0]\n",
    "    \n",
    "    \n",
    "    dealer_value_cards = np.sum(dealer_cards)\n",
    "    \n",
    "    # Stick if you have 21\n",
    "    if player_value_cards == 21 and dealer_value_cards != 21:\n",
    "        reward = 1\n",
    "        \n",
    "        hist_reward.append(reward)\n",
    "        hist_state.append((player_value_cards, has_usable_ace))\n",
    "        hist_action.append(1)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    # Strickly speaking, the policy should depend on:\n",
    "    #  1. The current state of the player, i.e., the value of her cards\n",
    "    #  2. The only card we observe of the dealer\n",
    "    # In this example, we consider a policy that only depends\n",
    "    # on the current value of the player's cards.\n",
    "\n",
    "    # Hit until you reach a 'stick' state or you lose (value of cards over 21)\n",
    "    while policy[player_value_cards - 12][1] != 1.0:\n",
    "        new_card = draw_card()\n",
    "        player_value_cards, new_has_usable_ace = update_player_card(player_value_cards, new_card)\n",
    "        has_usable_ace = has_usable_ace or new_has_usable_ace # keep usable ace if player already did have one.\n",
    "        hist_reward.append(0)\n",
    "        hist_action.append(0)\n",
    "        hist_state.append((player_value_cards, has_usable_ace))\n",
    "        \n",
    "        if player_value_cards > 21:\n",
    "            break\n",
    "            \n",
    "    dealer_value_cards = dealer_strategy(sum(dealer_cards))\n",
    "    \n",
    "    if player_value_cards > 21:\n",
    "        reward = -1\n",
    "    elif dealer_value_cards > 21:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 1 if player_value_cards > dealer_value_cards else 0\n",
    "    \n",
    "    hist_reward.append(reward)\n",
    "    \n",
    "    return reward, hist_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbd9d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We consider the policy that sticks if the \n",
    "# player's sum is 20 or 21, and otherwise hits\n",
    "policy = np.zeros((10, 2))\n",
    "policy[:-2, 0] = 1\n",
    "policy[-2:, 1] = 1\n",
    "policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "reinforcement-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
