{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab394903-07c2-4325-9189-80070f0f3350",
   "metadata": {},
   "source": [
    "# Blackjack - prediction\n",
    "\n",
    "In this notebook, we are interested in *learning* the value-function $v_\\pi(s)$ and action-value function $q_\\pi(s, a)$ for a given policy $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd929d34-5f36-4754-b653-f0f2171e6ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "debf50e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71627d2e",
   "metadata": {},
   "source": [
    "* **Stick**: Player stops\n",
    "* **Hit**: Request an additional card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f760e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vals = np.ones(10)\n",
    "n_vals[-1] = 4\n",
    "deck_probs = n_vals / n_vals.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "40cbb2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba.core import types\n",
    "from numba.typed import Dict\n",
    "\n",
    "types.float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "42d77a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def draw_card():\n",
    "    return np.random.multinomial(1, deck_probs).argmax()\n",
    "\n",
    "@jit(nopython=True)\n",
    "def dealer_strategy(value_cards):\n",
    "    \"\"\"\n",
    "    Dealer's fixed strategy\n",
    "    \"\"\"\n",
    "    while value_cards < 17:\n",
    "        value_cards = value_cards + draw_card()\n",
    "    return value_cards\n",
    "\n",
    "@jit(nopython=True)\n",
    "def blackjack(player_value_cards, dealer_cards, policy, has_usable_ace):\n",
    "    \"\"\"\n",
    "    For some reason, a player can have a minimum value of 12\n",
    "    on her initial value cards.\n",
    "    \n",
    "    At the start of the game, we are given the initial value of the cards\n",
    "    of the player, the initial dealer cards and a policy for the player.\n",
    "    Furtheremore, we are given whether the player has a usable ace.\n",
    "    \n",
    "    Actions:\n",
    "        0: hit\n",
    "        1: stick\n",
    "    \"\"\"\n",
    "    reward = 0\n",
    "    \n",
    "    hist_reward = [reward]\n",
    "    hist_state = [(player_value_cards, has_usable_ace)]\n",
    "    hist_action = [0]\n",
    "    \n",
    "    \n",
    "    dealer_value_cards = np.sum(dealer_cards)\n",
    "    \n",
    "    # Stick if you have 21\n",
    "    if player_value_cards == 21 and dealer_value_cards != 21:\n",
    "        reward = 1\n",
    "        \n",
    "        hist_reward.append(reward)\n",
    "        hist_state.append((player_value_cards, has_usable_ace))\n",
    "        hist_action.append(1)\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    # Strickly speaking, the policy should depend on:\n",
    "    #  1. The current state of the player, i.e., the value of her cards\n",
    "    #  2. The only card we observe of the dealer\n",
    "    # In this first notebook, we consider a policy that only depends\n",
    "    # on the current value of the player's cards\n",
    "\n",
    "    # Hit until you reach a 'stick' state or you lose\n",
    "    while policy[player_value_cards - 12][1] != 1.0:\n",
    "        player_value_cards = player_value_cards + draw_card()\n",
    "        hist_reward.append(0)\n",
    "        hist_state.append((player_value_cards, has_usable_ace))\n",
    "        hist_action.append(0)\n",
    "        \n",
    "        if player_value_cards > 21:\n",
    "            break\n",
    "            \n",
    "    dealer_value_cards = dealer_strategy(sum(dealer_cards))\n",
    "    \n",
    "    if player_value_cards > 21:\n",
    "        reward = -1\n",
    "    elif dealer_value_cards > 21:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = 1 if player_value_cards > dealer_value_cards else 0\n",
    "    \n",
    "    hist_reward.append(reward)\n",
    "    \n",
    "    return reward, hist_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dbd9d67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We consider the policy that sticks if the \n",
    "# player's sum is 20 or 21, and otherwise hits\n",
    "policy = np.zeros((10, 2))\n",
    "policy[:-2, 0] = 1\n",
    "policy[-2:, 1] = 1\n",
    "policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reinforcement-learning",
   "language": "python",
   "name": "reinforcement-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
