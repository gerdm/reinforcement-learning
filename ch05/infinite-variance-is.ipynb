{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a298f9-0946-4b0b-874b-a4f4f0c236cf",
   "metadata": {},
   "source": [
    "# Importance sampling â€” infinite variance example\n",
    "\n",
    "We seek to estimate the value function under the policy $\\pi(\\text{left}|s) = 1$ when the simulations are being sampled according to $\\pi(\\text{left} | s) = 1/2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa0c6a-b8b4-4309-94d4-04d4c97a1e8d",
   "metadata": {},
   "source": [
    "The estimate of the value function $v_\\pi(s)$ under ordinary importance sampling is given by\n",
    "\n",
    "$$\n",
    "     V(s) = \\frac{\\sum_{t\\in{\\mathfrak T}(s)} \\rho_{t:T(t) -1}G_t}{| \\mathfrak{T}(s) |}\n",
    "$$\n",
    "with\n",
    "* $\\mathfrak{T}(s)$ the set of all time steps in which state $s$ is visited,\n",
    "* $T(t)$ the first time of termination following $t$\n",
    "* $G_t$ the return after $t$ up though $T(t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b9d86bf-82c8-46c4-97e7-0b0a567b9677",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ceb3b6e-dad6-4bde-9513-643fa39b7683",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = \"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84d79e84-681a-40b4-8823-8537f1e84cd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transition_matrix[left/right, state/end_state]\n",
    "transition_matrix = np.array([\n",
    "    [0.9, 0.1],\n",
    "    [0.0, 1.0]\n",
    "])\n",
    "\n",
    "\n",
    "reward_matrix = np.array([\n",
    "    [0.0, 1.0],\n",
    "    [-np.inf, 0.0]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "ae238901-7ff4-4c08-b67c-582441052b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@njit\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "\n",
    "@njit\n",
    "def step(state, policy):\n",
    "    pr_actions = policy[state]\n",
    "    action = np.random.multinomial(1, pvals=pr_actions).argmax()\n",
    "    \n",
    "    next_state = np.random.multinomial(1, pvals=transition_matrix[action]).argmax()\n",
    "    reward = reward_matrix[action, next_state]\n",
    "    \n",
    "    return action, next_state, reward\n",
    "\n",
    "@njit\n",
    "def episode(state, policy):\n",
    "    states, actions, rewards = [state], [], []\n",
    "    while state != 1:\n",
    "        action, state, reward = step(state, policy)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    states = np.array(states)\n",
    "    actions = np.array(actions)\n",
    "    rewards = np.array(rewards)\n",
    "    return states, actions, rewards\n",
    "\n",
    "\n",
    "# @njit\n",
    "def value_function_is(n_sims, policy_behaviour, policy_target):\n",
    "    rewards_all = []\n",
    "    rho_all = []\n",
    "    \n",
    "    for n in range(n_sims):\n",
    "        states, actions, returns = episode(0, policy_behaviour)\n",
    "        rewards = returns[::-1].cumsum()[::-1]\n",
    "        rho_vals = policy_target[states[:-1], actions] / policy_behaviour[states[:-1], actions]\n",
    "        \n",
    "        rewards_all.extend(rewards)\n",
    "        rho_all.extend(rho_vals)\n",
    "    \n",
    "    rewards_all = np.array(rewards_all)\n",
    "    rho_all = np.array(rho_all)\n",
    "    return rewards_all, rho_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "0bcdc7a6-7beb-4052-9650-95931ea44679",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_behaviour = np.zeros((\n",
    "    1, # non-terminal state\n",
    "    2, # left or right\n",
    "))\n",
    "\n",
    "policy_behaviour[0, 0] = 1/2\n",
    "policy_behaviour[0, 1] = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "defaa6e9-86a8-4736-ba0a-80e05c0452d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "policy_target = np.zeros_like(policy_behaviour)\n",
    "policy_target[0, 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "85c07b61-8eee-4fd4-bffb-0e4dfd760f1b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(111111)\n",
    "states, actions, returns = episode(0, policy_behaviour)\n",
    "rewards = returns[::-1].cumsum()[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "7bd1c1c9-1cce-45b2-80bc-996200bcdba9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "65a3c92d-47d2-4997-a535-ef75db52cbf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "e28d5391-1868-45d4-b609-9b7459256932",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rho_vals = policy_target[states[:-1], actions] / policy_behaviour[states[:-1], actions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b69bceb4-7530-4cce-89e7-320dd2102b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2.])"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "275435d2-d56d-42a6-9208-5c315572620f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rewards, rho = value_function_is(200_000, policy_behaviour, policy_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "701fb218-4aa8-4b8d-b3a7-a7a54082ce7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18222455883445868"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(rewards * rho).sum() / len(rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
